{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib as plt\n",
    "import seaborn as sb\n",
    "import graphviz\n",
    "import os\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get working directory\n",
    "PWD = os.getcwd()\n",
    "# Get output directory\n",
    "OUTPUT_DIR = os.path.join(PWD, 'output')\n",
    "# Get dataset from 'PWD/input'\n",
    "DATASET_DIR = os.path.join(PWD, 'input')\n",
    "# Get dataset path\n",
    "DATASET = os.path.join(DATASET_DIR, 'titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in titanic.csv as a DataFrame\n",
    "titanic = pd.read_csv(DATASET, index_col='PassengerId')\n",
    "\n",
    "def split_train_test(data, SPLIT_SIZE=0.8, column_label_name='Survived'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data: dataset to be split\n",
    "        SPLIT_SIZE: split size of train and test dataset sizes, default is 0.8\n",
    "        column_label_name: the label column to be extracted\n",
    "    Output:\n",
    "        X_train: training data\n",
    "        X_test: test data\n",
    "        y_train: training data's associated labels\n",
    "        y_test: test data's associated labels\n",
    "    \"\"\"\n",
    "    indices = np.random.permutation(len(data))\n",
    "    train_size = int(len(data) * SPLIT_SIZE)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    X_train = data.iloc[train_indices]\n",
    "    X_test = data.iloc[test_indices]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Age' column 0\n",
      "Number of NaN values in 'Embarked' column 0\n"
     ]
    }
   ],
   "source": [
    "# Convert any continuous features into binary features or categorical features by thresholding\n",
    "# Fill 'NaN' values in 'Age' with median\n",
    "age_median = titanic['Age'].median()\n",
    "titanic['Age'].fillna(age_median, inplace=True)\n",
    "num_of_nulls_age = titanic['Age'].isnull().sum()\n",
    "print(f'Number of NaN values in \\'Age\\' column {num_of_nulls_age}')\n",
    "\n",
    "# Fill 'nan' values in 'Embarked' with mode\n",
    "embarked_mode = titanic['Embarked'].mode()[0]\n",
    "titanic.fillna(embarked_mode, inplace=True)\n",
    "num_of_nulls_embarked = titanic['Embarked'].isnull().sum()\n",
    "print(f'Number of NaN values in \\'Embarked\\' column {num_of_nulls_embarked}')\n",
    "\n",
    "# Age --> 0-12 child, 12-18 teen, 18-64 adults, 64+ elderly\n",
    "modify_age_col = pd.DataFrame(titanic['Age'])\n",
    "modify_age_col = modify_age_col.apply(lambda x: pd.cut(x, [0, 12.0, 18.0, 64.0, 100.0], labels=['child', 'teen', 'adults', 'elderly']))\n",
    "\n",
    "# Fare --> 0-10 poor, 10-40 middle class, 40+ upper class (?)\n",
    "modify_fare_col = pd.DataFrame(titanic['Fare'])\n",
    "modify_fare_col = modify_fare_col.apply(lambda x: pd.cut(x, [-0.5, 10.0, 40.0, 600.0], labels=['poor', 'middle class', 'upper class']))\n",
    "\n",
    "# Put modified columns into original dataset\n",
    "titanic['Age'] = modify_age_col['Age']\n",
    "titanic['Fare'] = modify_fare_col['Fare']\n",
    "\n",
    "# Don't handle 'NaN' values in 'Cabin' column just to see how well decision trees handle that(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training data: (712, 8)\n",
      "The shape of test data: (179, 8)\n"
     ]
    }
   ],
   "source": [
    "# Get rid of 'Name', 'Ticket', 'Cabin' column\n",
    "titanic = titanic.drop('Name', axis=1)\n",
    "titanic = titanic.drop('Ticket', axis=1)\n",
    "titanic = titanic.drop('Cabin', axis=1)\n",
    "\n",
    "# Split data set into train and test set\n",
    "X_train, X_test = split_train_test(titanic)\n",
    "print(f'The shape of training data: {X_train.shape}')\n",
    "print(f'The shape of test data: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build trees of a depth maximum. A user will be able to specify the max depth\n",
    "class ID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Constructs an ID3 Decision Tree\n",
    "        Input:\n",
    "            dataset: The dataset to fit onto\n",
    "            max_depth: Specified max depth of the tree\n",
    "        Output:\n",
    "            null\n",
    "        Initializes:\n",
    "            self.max_depth: Max tree depth of Decision Tree object\n",
    "            self.true_pos: True positive metric\n",
    "            self.true_neg: True negative metric\n",
    "            self.false_pos: False positive metric\n",
    "            self.false_neg: False negative metric\n",
    "        \"\"\"\n",
    "        if ((max_depth != None) and (max_depth < 0)):\n",
    "            raise Exception('max_depth can\\'t be negative')\n",
    "        self.max_depth = max_depth\n",
    "        self.true_pos = 0\n",
    "        self.true_neg = 0\n",
    "        self.false_pos = 0\n",
    "        self.false_neg = 0\n",
    "        self.accuracy = 0.0\n",
    "        self.precision = 0.0\n",
    "        self.recall = 0.0\n",
    "        self.conf_matrix = []\n",
    "        \n",
    "    def fit(self, X, target_attribute='Survived'):\n",
    "        \"\"\"\n",
    "        Fits the ID3 Decision Tree to the data provided (X)\n",
    "        Input:\n",
    "            X: The dataset to fit on\n",
    "            target_attribute: The attribute whose value is to be predicted by the tree\n",
    "        Output:\n",
    "            Root tree Node object\n",
    "        \"\"\"\n",
    "        if (not(target_attribute in list(X.columns.values))):\n",
    "            raise Exception('Provided target_attribute does not exist in the dataset X')\n",
    "        if (not(isinstance(X, pd.DataFrame))):\n",
    "            raise Exception('X is not of type \\'pd.DataFrame\\'')\n",
    "        \n",
    "        # Compute information gain on whole dataset then call _helper(X, target_attribute, attributes - highestIG(A), 1)\n",
    "        \n",
    "        # List of all attributes in X\n",
    "        attributes = list(X.columns.values)\n",
    "        # Remove class labels from being considered in IG calculations\n",
    "        attributes.remove(target_attribute)\n",
    "        # Max IG and its corresponding attribute\n",
    "        root_gain, root_attribute = self._get_max_information_gain(X, target_attribute, attributes)\n",
    "        # List of root_attribute's unique values\n",
    "        list_unique_vals = list(X[root_attribute].unique())\n",
    "        # Initialize this ID3's root with root_attribute and it's unique values\n",
    "        self.root = Node(root_attribute, list_unique_vals)\n",
    "        # Duplicate attributes since it'll be altered throughout the course of the program\n",
    "        dup_attributes = attributes.copy()\n",
    "        # Remove root_attribute from being considered in further IG calculations\n",
    "        dup_attributes.remove(self.root.attribute)\n",
    "        \n",
    "        # Check if root is max depth allowed by user\n",
    "        if (self.max_depth == 0):\n",
    "            # Return single-node tree root, with label = most common value of target_attribute in examples\n",
    "            # Get number of positive instances in X\n",
    "            num_pos = len(X.loc[X[target_attribute] == 1])\n",
    "            # Get number of negative instances in X\n",
    "            num_neg = len(X.loc[X[target_attribute] == 0])\n",
    "            if (num_neg > num_pos):\n",
    "                self.root = Node('Death')\n",
    "                return self.root\n",
    "            else:\n",
    "                self.root = Node('Survived')\n",
    "                return self.root\n",
    "            \n",
    "        for root_value in self.root.values:\n",
    "            # X with all instances where root_attribute == root_value\n",
    "            split_df = X.loc[X[root_attribute] == root_value]\n",
    "            self.root.insert_edge(root_value, self._fit_helper(split_df, target_attribute, dup_attributes, 1))\n",
    "        return self.root\n",
    "        \n",
    "    def _fit_helper(self, X, target_attribute, attributes, cur_depth):\n",
    "        \"\"\"\n",
    "        Recurses down to a leaf node for each attribute and its attributes's values and builds a Node to be connected to the root.\n",
    "        Input:\n",
    "            X: Dataset whether previously partitioned or not\n",
    "            target_attribute: The attribute whose value is to be predicted by the tree\n",
    "            attributes: A list of all the attributes under examination after partitions\n",
    "        Output:\n",
    "            Child Node object connected to root\n",
    "        \"\"\"        \n",
    "        root = Node()\n",
    "        # Get number of positive instances in X\n",
    "        num_pos = len(X.loc[X[target_attribute] == 1])\n",
    "        # Get number of negative instances in X\n",
    "        num_neg = len(X.loc[X[target_attribute] == 0])\n",
    "        \n",
    "        # If cur_depth == max_depth, return single-node tree root, with label = most common value of target_attribute in examples\n",
    "        if (cur_depth == self.max_depth):\n",
    "            if (num_neg > num_pos):\n",
    "                root.attribute = 'Death'\n",
    "                return root\n",
    "            else:\n",
    "                root.attribute = 'Survived'\n",
    "                return root\n",
    "        # If all examples are positive, return the single-node tree root, with label = +\n",
    "        if (num_neg == 0 and num_pos > 0):\n",
    "            root.attribute = 'Survived'\n",
    "            return root\n",
    "        # If all examples are negative, return the single-node tree root, with label = -\n",
    "        if (num_pos == 0 and num_neg > 0):\n",
    "            root.attribute = 'Death'\n",
    "            return root\n",
    "        # If attributes is empty, return the single-node tree root, with label = most common value of target_attribute in examples\n",
    "        if (len(attributes) == 0):\n",
    "            if (num_neg > num_pos):\n",
    "                root.attribute = 'Death'\n",
    "                return root\n",
    "            else:\n",
    "                root.attribute = 'Survived'\n",
    "                return root\n",
    "            \n",
    "        root_gain, root_attribute = self._get_max_information_gain(X, target_attribute, attributes)\n",
    "        # List of root_attribute's unique values\n",
    "        list_unique_vals = list(X[root_attribute].unique())\n",
    "        # Initialize this ID3's root with root_attribute and it's unique values\n",
    "        root.attribute = root_attribute\n",
    "        root.values = list_unique_vals\n",
    "        # Duplicate attributes since it'll be altered throughout the course of the program\n",
    "        dup_attributes = attributes.copy()\n",
    "        # Remove root_attribute from being considered in further IG calculations\n",
    "        dup_attributes.remove(root.attribute)\n",
    "        for root_value in root.values:\n",
    "            # X with all instances where root_attribute == root_value\n",
    "            split_df = X.loc[X[root_attribute] == root_value]\n",
    "            # If examples split on the ith value of root_attribute == 0, return single-node tree root, with label = most common value of target_attribute in examples\n",
    "            if (len(split_df) == 0):\n",
    "                label = ''\n",
    "                if (num_neg > num_pos):\n",
    "                    label = 'Death'\n",
    "                else:\n",
    "                    label = 'Survived'\n",
    "                leaf = Node(label)\n",
    "                return root.insert_edge(leaf)\n",
    "            else:\n",
    "                root.insert_edge(root_value, self._fit_helper(split_df, target_attribute, dup_attributes, cur_depth + 1))\n",
    "        return root\n",
    "                \n",
    "    def _get_max_information_gain(self, X, target_attribute, attributes):\n",
    "        \"\"\"\n",
    "        Returns max information gain\n",
    "        Input:\n",
    "            X: Dataset whether previously partitioned or not\n",
    "            attributes: A list of the attributes for the the dataset provided\n",
    "        Output:\n",
    "            max_gain: The max information gain for the dataset\n",
    "            partition_attribute: The attribute to split on\n",
    "        \"\"\"\n",
    "        max_gain = float(\"-inf\") # Max info. gain variable\n",
    "        partition_attribute = None # The string name of the attribute with max info. gain\n",
    "        neg_examples = len(X.loc[X[target_attribute] == 1, target_attribute]) # Number of negative examples\n",
    "        pos_examples = len(X.loc[X[target_attribute] == 0, target_attribute]) # Number of positive examples\n",
    "        examples = neg_examples + pos_examples # Total examples\n",
    "        whole_entropy = -((pos_examples/examples) * math.log2(pos_examples/examples)) - ((neg_examples/examples) * math.log2(neg_examples/examples))\n",
    "        \n",
    "        for attribute in attributes:\n",
    "            # Keep track of max\n",
    "            max_so_far = whole_entropy\n",
    "            # Get list of values for the attribute\n",
    "            values = X[attribute].unique() \n",
    "            for value in values:\n",
    "                # Get number of positive examples for this attribute's value\n",
    "                pos_val_examples = len(X.loc[(X[target_attribute] == 1) & (X[attribute] == value), target_attribute])\n",
    "                # Get number of negative examples for this attribute's value\n",
    "                neg_val_examples = len(X.loc[(X[target_attribute] == 0) & (X[attribute] == value), target_attribute])\n",
    "                # Get total number of examples for this attribute's value\n",
    "                value_examples = neg_val_examples + pos_val_examples\n",
    "                value_entropy = 0\n",
    "                \n",
    "                try:\n",
    "                    value_entropy = -((pos_val_examples/value_examples) * math.log2(pos_val_examples/value_examples)) - ((neg_val_examples/value_examples) * math.log2(neg_val_examples/value_examples))\n",
    "                except:\n",
    "                    if ((pos_val_examples/value_examples) == 0.0 and (neg_val_examples/value_examples) == 0):\n",
    "                        value_entropy = 0\n",
    "                    elif ((pos_val_examples/value_examples) == 0.0):\n",
    "                        value_entropy = -((neg_val_examples/value_examples) * math.log2(neg_val_examples/value_examples))\n",
    "                    else:\n",
    "                        value_entropy = -((pos_val_examples/value_examples) * math.log2(pos_val_examples/value_examples))\n",
    "                \n",
    "                value_ratio = value_examples / examples\n",
    "                max_so_far -= ((value_ratio) * (value_entropy))\n",
    "            if (max_so_far > max_gain):\n",
    "                max_gain = max_so_far\n",
    "                partition_attribute = attribute\n",
    "        return max_gain, partition_attribute\n",
    "    \n",
    "    def predict(self, X, target_attribute=\"Survived\"):\n",
    "        \"\"\"\n",
    "        Take X to test this ID3 on, and initalize metrics\n",
    "        Input:\n",
    "            X: The dataset to test this ID3 on\n",
    "            target_attribute: The name of the label column used to measure predictions\n",
    "        Output:\n",
    "            self.accuracy: Computes the accuracy after predictions have been done and returns it\n",
    "        Initialize:\n",
    "            self.true_pos: Increments self.true_pos for every correct positive example we classify\n",
    "            self.true_neg: Increments self.true_neg for every correct negative example we classify\n",
    "            self.false_pos: Increments self.false_pos for every incorrect positive example we classify\n",
    "            self.false_neg: Increments self.false_neg for every incorrect negative example we classify\n",
    "            self.conf_matrix: A 'c x c' dimensional matrix with C_ij is equal to the number of observations to be in group i and predicted to be in group j\n",
    "        \"\"\"\n",
    "        for index, row in X.iterrows():\n",
    "            instance_label = row[target_attribute]\n",
    "            node = self.root\n",
    "            node_attrib = node.attribute\n",
    "            while (node.edges):\n",
    "                # Get value of the instance's attribute that corresponds to the root attribute\n",
    "                attrib_value = row[node_attrib]\n",
    "                # Update node\n",
    "                if (attrib_value in node.edges):\n",
    "                    node = node.edges[attrib_value]\n",
    "                else:\n",
    "                    # Attribute value does not exist in this decision tree path\n",
    "                    self.false_pos += 1\n",
    "                    break\n",
    "                # Update the attribute at the updated node\n",
    "                node_attrib = node.attribute\n",
    "                \n",
    "            if (node.attribute == 'Survived' and instance_label == 1):\n",
    "                # True positive\n",
    "                self.true_pos += 1\n",
    "            elif (node.attribute == 'Death' and instance_label == 0):\n",
    "                # True negative\n",
    "                self.true_neg += 1\n",
    "            elif (node.attribute == 'Survived' and instance_label == 0):\n",
    "                # False positive\n",
    "                self.false_pos += 1\n",
    "            elif (node.attribute == 'Death' and instance_label == 1):\n",
    "                # False negative\n",
    "                self.false_neg += 1\n",
    "        self.conf_matrix = [[self.true_neg, self.false_pos], [self.false_neg, self.true_pos]]\n",
    "        self.accuracy = (self.true_pos + self.true_neg) / (self.true_pos + self.true_neg + self.false_pos + self.false_neg)\n",
    "        self.precision = (self.true_pos) / (self.true_pos + self.false_pos)\n",
    "        self.recall = (self.true_pos) / (self.true_pos + self.false_neg)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def export_graphviz(self, SAVE_DIR=OUTPUT_DIR, filename='id3.sv'):\n",
    "        \"\"\"\n",
    "        Saves the visualization of this ID3's tree structure and renders the image\n",
    "        \"\"\"\n",
    "        self.digraph = graphviz.Digraph()\n",
    "        root = self.root \n",
    "        self.digraph.node(str(root), str(root.attribute))\n",
    "        for key, value in root.edges.items():\n",
    "            self.digraph.node(str(value), str(value.attribute))\n",
    "            self.digraph.edge(str(root), str(value), label=str(key))\n",
    "            self._export_graphviz_helper(value)\n",
    "        self.digraph.render(os.path.join(SAVE_DIR, filename), view=True)\n",
    "    \n",
    "    def _export_graphviz_helper(self, root):\n",
    "        \"\"\"\n",
    "        Passed a Node object and connects the rest of the subtree to root parameter\n",
    "        Input:\n",
    "            Node object to connect the rest of the subtree too\n",
    "        Output:\n",
    "            null\n",
    "        \"\"\"\n",
    "        if (not root.edges):\n",
    "            # If no edges then store node with root.attribute's value of 'Survived' or 'Death'\n",
    "            return\n",
    "        else:\n",
    "            for k, v in root.edges.items():\n",
    "                self.digraph.node(str(v), str(v.attribute))\n",
    "                self.digraph.edge(str(root), str(v), label=str(k))\n",
    "                self._export_graphviz_helper(v)\n",
    "        \n",
    "    def __str__(self):\n",
    "        pass\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, attribute=None, values=None, attribute_value=None):\n",
    "        \"\"\"\n",
    "        Constructs a tree Node\n",
    "        Input:\n",
    "            attribute: The attribute we split the data on\n",
    "            values: A list of all unique values of the given attribute\n",
    "        Output:\n",
    "            null\n",
    "        Initializes:\n",
    "            self.attribute: Value of this Node oject\n",
    "            self.values: Values associated with this Node's value (a.k.a attribute)\n",
    "            self.edges: A list of all the edges this attribute Node connects too\n",
    "        \"\"\"\n",
    "        self.attribute = attribute\n",
    "        self.values = values\n",
    "        self.edges = {}\n",
    "        \n",
    "    def insert_edge(self, attribute_value, node):\n",
    "        self.edges[attribute_value] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small data\n",
    "play_tennis_data = pd.read_csv(os.path.join(os.path.join(DATASET_DIR, 'play-tennis.csv')))\n",
    "play_tennis = ID3()\n",
    "play_tennis_root = play_tennis.fit(play_tennis_data, 'play-tennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fit on X_train dataset\n",
    "id3 = ID3()\n",
    "root = id3.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the learned tree as a both object code and as a visual interpretable model\n",
    "# Object code\n",
    "pickle.dump(id3, open(os.path.join(OUTPUT_DIR, 'id3.sav'), 'wb'))\n",
    "loaded_model = pickle.load(open(os.path.join(OUTPUT_DIR, 'id3.sav'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphviz\n",
    "play_tennis.export_graphviz(OUTPUT_DIR, 'play-tennis.sv')\n",
    "id3.export_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3's true positive: 44\n",
      "ID3's true negative: 94\n",
      "ID3's false positive: 25\n",
      "ID3's false negative: 16\n",
      "ID3's accuracy: 0.770949720670391\n",
      "ID3's precision: 0.6376811594202898\n",
      "ID3's recall: 0.7333333333333333\n",
      "ID3's confusion matrix; [[94, 25], [16, 44]]\n"
     ]
    }
   ],
   "source": [
    "# Output a confusion matrix from the constructed tree on the test set. Compute accuracy, precision, and recall. \n",
    "id3_accuracy = id3.predict(X_test)\n",
    "print(f'ID3\\'s true positive: {id3.true_pos}')\n",
    "print(f'ID3\\'s true negative: {id3.true_neg}')\n",
    "print(f'ID3\\'s false positive: {id3.false_pos}')\n",
    "print(f'ID3\\'s false negative: {id3.false_neg}')\n",
    "print(f'ID3\\'s accuracy: {id3.accuracy}')\n",
    "print(f'ID3\\'s precision: {id3.precision}')\n",
    "print(f'ID3\\'s recall: {id3.recall}')\n",
    "print(f'ID3\\'s confusion matrix; {id3.conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's confusion matrix: [[93 18]\n",
      " [21 47]]\n",
      "sklearn's accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "# Compare your coded tree's metrics with standard Scikit-Learn's decision tree implementation.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sklearn_X_train = X_train.copy()\n",
    "sklearn_y_train = sklearn_X_train.pop('Survived')\n",
    "sklearn_X_test = X_test.copy()\n",
    "sklearn_y_test = sklearn_X_test.pop('Survived')\n",
    "\n",
    "# Preprocess for sklearn Decision Tree\n",
    "# Sex column\n",
    "sklearn_X_train['Sex'] = sklearn_X_train['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "sklearn_X_test['Sex'] = sklearn_X_test['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "# Age column\n",
    "sklearn_X_train['Age'] = sklearn_X_train['Age'].map({'child': 0, 'teen': 1, 'adults': 2, 'elderly':3}).astype(int)\n",
    "sklearn_X_test['Age'] = sklearn_X_test['Age'].map({'child': 0, 'teen': 1, 'adults': 2, 'elderly':3}).astype(int)\n",
    "# Fare column\n",
    "sklearn_X_train['Fare'] = sklearn_X_train['Fare'].map({'poor': 0, 'middle class': 1, 'upper class': 2}).astype(int)\n",
    "sklearn_X_test['Fare'] = sklearn_X_test['Fare'].map({'poor': 0, 'middle class': 1, 'upper class': 2}).astype(int)\n",
    "# Embarked column\n",
    "sklearn_X_train['Embarked'] = sklearn_X_train['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "sklearn_X_test['Embarked'] = sklearn_X_test['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "\n",
    "\n",
    "dec_clf = DecisionTreeClassifier('entropy')\n",
    "dec_clf.fit(sklearn_X_train, sklearn_y_train)\n",
    "predictions = dec_clf.predict(sklearn_X_test)\n",
    "sklearn_conf_matrix = confusion_matrix(sklearn_y_test, predictions)\n",
    "print(f'sklearn\\'s confusion matrix: {sklearn_conf_matrix}')\n",
    "print(f'sklearn\\'s accuracy: {(sklearn_conf_matrix[0][0] + sklearn_conf_matrix[-1][-1]) / (sklearn_conf_matrix[0][0] + sklearn_conf_matrix[0][-1] + sklearn_conf_matrix[-1][0] + sklearn_conf_matrix[-1][-1])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
